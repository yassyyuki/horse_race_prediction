{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f8c1c6",
   "metadata": {},
   "source": [
    "# ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6bdb203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from itertools import combinations, permutations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee310bd2",
   "metadata": {},
   "source": [
    "# 前処理済みのデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a0587f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>単勝</th>\n",
       "      <th>course_len</th>\n",
       "      <th>date</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>齢</th>\n",
       "      <th>...</th>\n",
       "      <th>race_type_芝</th>\n",
       "      <th>race_type_ダート</th>\n",
       "      <th>race_type_障害</th>\n",
       "      <th>ground_state_良</th>\n",
       "      <th>ground_state_稍重</th>\n",
       "      <th>ground_state_不良</th>\n",
       "      <th>ground_state_重</th>\n",
       "      <th>性_牡</th>\n",
       "      <th>性_牝</th>\n",
       "      <th>性_セ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201001010101</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>54.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>2008100961</td>\n",
       "      <td>1015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201001010101</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>2008104484</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              枠番  馬番    斤量    単勝  course_len        date    horse_id  \\\n",
       "201001010101   8  10  54.0  13.7        18.0  2010-08-14  2008100961   \n",
       "201001010101   6   7  54.0   7.6        18.0  2010-08-14  2008104484   \n",
       "\n",
       "              jockey_id  rank  齢  ...  race_type_芝  race_type_ダート  \\\n",
       "201001010101       1015     1  2  ...            1              0   \n",
       "201001010101       1091     1  2  ...            1              0   \n",
       "\n",
       "              race_type_障害  ground_state_良  ground_state_稍重  ground_state_不良  \\\n",
       "201001010101             0               1                0                0   \n",
       "201001010101             0               1                0                0   \n",
       "\n",
       "              ground_state_重  性_牡  性_牝  性_セ  \n",
       "201001010101               0    1    0    0  \n",
       "201001010101               0    1    0    0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./csv/preprocessed_df.csv', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080f326",
   "metadata": {},
   "source": [
    "# データの分割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e537a4",
   "metadata": {},
   "source": [
    "時系列データなので、ランダムにシャッフルすることなく、データを分割する。<br>\n",
    "30%をテストデータにする方針でコードを書く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d25211ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#時系列に沿って訓練データとテストデータに分ける関数\n",
    "def split_data(df, test_size=0.3):\n",
    "    sorted_id_list = df.sort_values(\"date\").index.unique()\n",
    "    train_id_list = sorted_id_list[: round(len(sorted_id_list) * (1 - test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list) * (1 - test_size)) :]\n",
    "    train = df.loc[train_id_list]\n",
    "    test = df.loc[test_id_list]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b09bf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b94831bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>単勝</th>\n",
       "      <th>course_len</th>\n",
       "      <th>date</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>齢</th>\n",
       "      <th>...</th>\n",
       "      <th>race_type_芝</th>\n",
       "      <th>race_type_ダート</th>\n",
       "      <th>race_type_障害</th>\n",
       "      <th>ground_state_良</th>\n",
       "      <th>ground_state_稍重</th>\n",
       "      <th>ground_state_不良</th>\n",
       "      <th>ground_state_重</th>\n",
       "      <th>性_牡</th>\n",
       "      <th>性_牝</th>\n",
       "      <th>性_セ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>2005106677</td>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>2005103849</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              枠番  馬番    斤量    単勝  course_len        date    horse_id  \\\n",
       "201006010108   7  12  57.0   4.2        18.0  2010-01-05  2005106677   \n",
       "201006010108   3   4  57.0  14.9        18.0  2010-01-05  2005103849   \n",
       "\n",
       "              jockey_id  rank  齢  ...  race_type_芝  race_type_ダート  \\\n",
       "201006010108        422     1  5  ...            0              1   \n",
       "201006010108       1065     1  5  ...            0              1   \n",
       "\n",
       "              race_type_障害  ground_state_良  ground_state_稍重  ground_state_不良  \\\n",
       "201006010108             0               1                0                0   \n",
       "201006010108             0               1                0                0   \n",
       "\n",
       "              ground_state_重  性_牡  性_牝  性_セ  \n",
       "201006010108               0    1    0    0  \n",
       "201006010108               0    1    0    0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "14e24906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>単勝</th>\n",
       "      <th>course_len</th>\n",
       "      <th>date</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>齢</th>\n",
       "      <th>...</th>\n",
       "      <th>race_type_芝</th>\n",
       "      <th>race_type_ダート</th>\n",
       "      <th>race_type_障害</th>\n",
       "      <th>ground_state_良</th>\n",
       "      <th>ground_state_稍重</th>\n",
       "      <th>ground_state_不良</th>\n",
       "      <th>ground_state_重</th>\n",
       "      <th>性_牡</th>\n",
       "      <th>性_牝</th>\n",
       "      <th>性_セ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201909010409</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>2015106194</td>\n",
       "      <td>1093</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010409</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>2015102499</td>\n",
       "      <td>1102</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              枠番  馬番    斤量    単勝  course_len        date    horse_id  \\\n",
       "201909010409   4   8  57.0  16.0        14.0  2019-03-03  2015106194   \n",
       "201909010409   7  14  57.0   4.5        14.0  2019-03-03  2015102499   \n",
       "\n",
       "              jockey_id  rank  齢  ...  race_type_芝  race_type_ダート  \\\n",
       "201909010409       1093     1  4  ...            0              1   \n",
       "201909010409       1102     1  4  ...            0              1   \n",
       "\n",
       "              race_type_障害  ground_state_良  ground_state_稍重  ground_state_不良  \\\n",
       "201909010409             0               0                1                0   \n",
       "201909010409             0               0                1                0   \n",
       "\n",
       "              ground_state_重  性_牡  性_牝  性_セ  \n",
       "201909010409               0    1    0    0  \n",
       "201909010409               0    0    0    1  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d15aa",
   "metadata": {},
   "source": [
    "# 訓練データをさらに訓練データ・検証データに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a4fa69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64fadd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['枠番', '馬番', '斤量', '単勝', 'course_len', 'date', 'horse_id', 'jockey_id',\n",
       "       'rank', '齢', '体重', '体重変化', '開催', 'n_horses', 'jockey_label',\n",
       "       'horse_label', 'weather_晴', 'weather_曇', 'weather_小雨', 'weather_雨',\n",
       "       'weather_小雪', 'weather_雪', 'race_type_芝', 'race_type_ダート',\n",
       "       'race_type_障害', 'ground_state_良', 'ground_state_稍重', 'ground_state_不良',\n",
       "       'ground_state_重', '性_牡', '性_牝', '性_セ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "053e6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要なカラムの削除と、目的変数の切り分け（単勝オッズは抜いてみた）\n",
    "# X_train = train_data.drop(['date', '単勝', 'rank'], axis=1)\n",
    "# y_train = train_data['rank']\n",
    "# X_valid = valid_data.drop(['date', '単勝', 'rank'], axis=1)\n",
    "# y_valid = valid_data['rank']\n",
    "\n",
    "\n",
    "# 不要なカラムの削除と、目的変数の切り分け（単勝オッズは抜いてみた）\n",
    "X_train = train_data.drop(['date', 'rank'], axis=1)\n",
    "y_train = train_data['rank']\n",
    "X_valid = valid_data.drop(['date', 'rank'], axis=1)\n",
    "y_valid = valid_data['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85541e0",
   "metadata": {},
   "source": [
    "# optunaでパラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9cf39c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-13 22:53:03,045]\u001b[0m A new study created in memory with name: no-name-5955ac58-4640-48df-9f83-b66ff3625992\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|               | 0/7 [00:00<?, ?it/s]/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401151:  14%|2 | 1/7 [00:01<00:09,  1.62s/it]\u001b[32m[I 2023-02-13 22:53:04,677]\u001b[0m Trial 0 finished with value: 0.4011513317541611 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.4011513317541611.\u001b[0m\n",
      "feature_fraction, val_score: 0.401151:  14%|2 | 1/7 [00:01<00:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's binary_logloss: 0.397178\tvalid_1's binary_logloss: 0.401151\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401136:  29%|5 | 2/7 [00:03<00:07,  1.51s/it]\u001b[32m[I 2023-02-13 22:53:06,106]\u001b[0m Trial 1 finished with value: 0.4011358191544124 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.4011358191544124.\u001b[0m\n",
      "feature_fraction, val_score: 0.401136:  29%|5 | 2/7 [00:03<00:07,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's binary_logloss: 0.396992\tvalid_1's binary_logloss: 0.401136\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401119:  43%|8 | 3/7 [00:04<00:05,  1.49s/it]\u001b[32m[I 2023-02-13 22:53:07,579]\u001b[0m Trial 2 finished with value: 0.40111906105025447 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 2 with value: 0.40111906105025447.\u001b[0m\n",
      "feature_fraction, val_score: 0.401119:  43%|8 | 3/7 [00:04<00:05,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's binary_logloss: 0.396943\tvalid_1's binary_logloss: 0.401119\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401119:  57%|#1| 4/7 [00:06<00:04,  1.58s/it]\u001b[32m[I 2023-02-13 22:53:09,303]\u001b[0m Trial 3 finished with value: 0.4012201488500041 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 0.40111906105025447.\u001b[0m\n",
      "feature_fraction, val_score: 0.401119:  57%|#1| 4/7 [00:06<00:04,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's binary_logloss: 0.39652\tvalid_1's binary_logloss: 0.40122\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.394982\tvalid_1's binary_logloss: 0.401473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401119:  71%|#4| 5/7 [00:08<00:03,  1.96s/it]\u001b[32m[I 2023-02-13 22:53:11,942]\u001b[0m Trial 4 finished with value: 0.40140602263350794 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.40111906105025447.\u001b[0m\n",
      "feature_fraction, val_score: 0.401119:  71%|#4| 5/7 [00:08<00:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's binary_logloss: 0.394124\tvalid_1's binary_logloss: 0.401406\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401119:  86%|#7| 6/7 [00:10<00:01,  2.00s/it]\u001b[32m[I 2023-02-13 22:53:14,003]\u001b[0m Trial 5 finished with value: 0.40121769902421256 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.40111906105025447.\u001b[0m\n",
      "feature_fraction, val_score: 0.401119:  86%|#7| 6/7 [00:10<00:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.396075\tvalid_1's binary_logloss: 0.401218\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.401119: 100%|##| 7/7 [00:13<00:00,  2.02s/it]\u001b[32m[I 2023-02-13 22:53:16,071]\u001b[0m Trial 6 finished with value: 0.40150218310658614 and parameters: {'feature_fraction': 0.5}. Best is trial 2 with value: 0.40111906105025447.\u001b[0m\n",
      "feature_fraction, val_score: 0.401119: 100%|##| 7/7 [00:13<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.396283\tvalid_1's binary_logloss: 0.401502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.401119:   0%|               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.401119:   5%|3      | 1/20 [00:01<00:28,  1.51s/it]\u001b[32m[I 2023-02-13 22:53:17,591]\u001b[0m Trial 7 finished with value: 0.4021757724394245 and parameters: {'num_leaves': 140}. Best is trial 7 with value: 0.4021757724394245.\u001b[0m\n",
      "num_leaves, val_score: 0.401119:   5%|3      | 1/20 [00:01<00:28,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.38961\tvalid_1's binary_logloss: 0.402176\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.401119:  10%|7      | 2/20 [00:03<00:27,  1.52s/it]\u001b[32m[I 2023-02-13 22:53:19,115]\u001b[0m Trial 8 finished with value: 0.40169975841375793 and parameters: {'num_leaves': 72}. Best is trial 8 with value: 0.40169975841375793.\u001b[0m\n",
      "num_leaves, val_score: 0.401119:  10%|7      | 2/20 [00:03<00:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's binary_logloss: 0.393472\tvalid_1's binary_logloss: 0.4017\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.401119:  15%|#      | 3/20 [00:06<00:38,  2.27s/it]\u001b[32m[I 2023-02-13 22:53:22,276]\u001b[0m Trial 9 finished with value: 0.40209536545474656 and parameters: {'num_leaves': 110}. Best is trial 8 with value: 0.40169975841375793.\u001b[0m\n",
      "num_leaves, val_score: 0.401119:  15%|#      | 3/20 [00:06<00:38,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.392253\tvalid_1's binary_logloss: 0.402095\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.401119:  20%|#4     | 4/20 [00:10<00:47,  2.99s/it]\u001b[32m[I 2023-02-13 22:53:26,386]\u001b[0m Trial 10 finished with value: 0.40292447113921825 and parameters: {'num_leaves': 217}. Best is trial 8 with value: 0.40169975841375793.\u001b[0m\n",
      "num_leaves, val_score: 0.401119:  20%|#4     | 4/20 [00:10<00:47,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.384713\tvalid_1's binary_logloss: 0.402924\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399627\tvalid_1's binary_logloss: 0.400813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  25%|#7     | 5/20 [00:13<00:43,  2.89s/it]\u001b[32m[I 2023-02-13 22:53:29,099]\u001b[0m Trial 11 finished with value: 0.400599761123985 and parameters: {'num_leaves': 3}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  25%|#7     | 5/20 [00:13<00:43,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's binary_logloss: 0.399361\tvalid_1's binary_logloss: 0.4006\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  30%|##1    | 6/20 [00:15<00:39,  2.80s/it]\u001b[32m[I 2023-02-13 22:53:31,727]\u001b[0m Trial 12 finished with value: 0.4011103291430912 and parameters: {'num_leaves': 33}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  30%|##1    | 6/20 [00:15<00:39,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's binary_logloss: 0.396299\tvalid_1's binary_logloss: 0.40111\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  35%|##4    | 7/20 [00:19<00:38,  2.99s/it]\u001b[32m[I 2023-02-13 22:53:35,116]\u001b[0m Trial 13 finished with value: 0.4026201629222202 and parameters: {'num_leaves': 173}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  35%|##4    | 7/20 [00:19<00:38,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.387713\tvalid_1's binary_logloss: 0.40262\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  40%|##8    | 8/20 [00:23<00:41,  3.42s/it]\u001b[32m[I 2023-02-13 22:53:39,457]\u001b[0m Trial 14 finished with value: 0.40296526580427605 and parameters: {'num_leaves': 212}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  40%|##8    | 8/20 [00:23<00:41,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.384967\tvalid_1's binary_logloss: 0.402965\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  45%|###1   | 9/20 [00:25<00:33,  3.09s/it]\u001b[32m[I 2023-02-13 22:53:41,817]\u001b[0m Trial 15 finished with value: 0.40114730885367805 and parameters: {'num_leaves': 36}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  45%|###1   | 9/20 [00:25<00:33,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's binary_logloss: 0.396872\tvalid_1's binary_logloss: 0.401147\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  50%|###   | 10/20 [00:28<00:30,  3.10s/it]\u001b[32m[I 2023-02-13 22:53:44,937]\u001b[0m Trial 16 finished with value: 0.4024869745494808 and parameters: {'num_leaves': 148}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  50%|###   | 10/20 [00:28<00:30,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.388128\tvalid_1's binary_logloss: 0.402487\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.400218\tvalid_1's binary_logloss: 0.401421\n",
      "[200]\tvalid_0's binary_logloss: 0.399666\tvalid_1's binary_logloss: 0.400823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400600:  55%|###3  | 11/20 [00:31<00:27,  3.09s/it]\u001b[32m[I 2023-02-13 22:53:47,992]\u001b[0m Trial 17 finished with value: 0.4007041274052245 and parameters: {'num_leaves': 2}. Best is trial 11 with value: 0.400599761123985.\u001b[0m\n",
      "num_leaves, val_score: 0.400600:  55%|###3  | 11/20 [00:31<00:27,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's binary_logloss: 0.399491\tvalid_1's binary_logloss: 0.400711\n",
      "Early stopping, best iteration is:\n",
      "[297]\tvalid_0's binary_logloss: 0.399495\tvalid_1's binary_logloss: 0.400704\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  60%|###6  | 12/20 [00:34<00:22,  2.86s/it]\u001b[32m[I 2023-02-13 22:53:50,348]\u001b[0m Trial 18 finished with value: 0.40053594829665545 and parameters: {'num_leaves': 9}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  60%|###6  | 12/20 [00:34<00:22,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.398522\tvalid_1's binary_logloss: 0.400536\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.400218\tvalid_1's binary_logloss: 0.401421\n",
      "[200]\tvalid_0's binary_logloss: 0.399666\tvalid_1's binary_logloss: 0.400823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  65%|###9  | 13/20 [00:37<00:19,  2.83s/it]\u001b[32m[I 2023-02-13 22:53:53,088]\u001b[0m Trial 19 finished with value: 0.4007041274052245 and parameters: {'num_leaves': 2}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  65%|###9  | 13/20 [00:37<00:19,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's binary_logloss: 0.399491\tvalid_1's binary_logloss: 0.400711\n",
      "Early stopping, best iteration is:\n",
      "[297]\tvalid_0's binary_logloss: 0.399495\tvalid_1's binary_logloss: 0.400704\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  70%|####1 | 14/20 [00:38<00:15,  2.52s/it]\u001b[32m[I 2023-02-13 22:53:54,899]\u001b[0m Trial 20 finished with value: 0.4016221329476199 and parameters: {'num_leaves': 77}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  70%|####1 | 14/20 [00:38<00:15,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.393229\tvalid_1's binary_logloss: 0.401622\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  75%|####5 | 15/20 [00:40<00:11,  2.23s/it]\u001b[32m[I 2023-02-13 22:53:56,471]\u001b[0m Trial 21 finished with value: 0.40156043837957106 and parameters: {'num_leaves': 51}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  75%|####5 | 15/20 [00:40<00:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.395841\tvalid_1's binary_logloss: 0.40156\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  80%|####8 | 16/20 [00:42<00:08,  2.24s/it]\u001b[32m[I 2023-02-13 22:53:58,737]\u001b[0m Trial 22 finished with value: 0.4029509344501884 and parameters: {'num_leaves': 254}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  80%|####8 | 16/20 [00:42<00:08,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.382307\tvalid_1's binary_logloss: 0.402951\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  85%|#####1| 17/20 [00:44<00:06,  2.19s/it]\u001b[32m[I 2023-02-13 22:54:00,807]\u001b[0m Trial 23 finished with value: 0.40184569653936586 and parameters: {'num_leaves': 85}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  85%|#####1| 17/20 [00:44<00:06,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.392928\tvalid_1's binary_logloss: 0.401846\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  90%|#####4| 18/20 [00:47<00:04,  2.38s/it]\u001b[32m[I 2023-02-13 22:54:03,636]\u001b[0m Trial 24 finished with value: 0.4010650645313454 and parameters: {'num_leaves': 24}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  90%|#####4| 18/20 [00:47<00:04,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's binary_logloss: 0.397333\tvalid_1's binary_logloss: 0.401065\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536:  95%|#####6| 19/20 [00:50<00:02,  2.50s/it]\u001b[32m[I 2023-02-13 22:54:06,395]\u001b[0m Trial 25 finished with value: 0.40199483174079803 and parameters: {'num_leaves': 98}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536:  95%|#####6| 19/20 [00:50<00:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's binary_logloss: 0.390767\tvalid_1's binary_logloss: 0.401995\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.400536: 100%|######| 20/20 [00:52<00:00,  2.50s/it]\u001b[32m[I 2023-02-13 22:54:08,916]\u001b[0m Trial 26 finished with value: 0.4014523335814803 and parameters: {'num_leaves': 57}. Best is trial 18 with value: 0.40053594829665545.\u001b[0m\n",
      "num_leaves, val_score: 0.400536: 100%|######| 20/20 [00:52<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.393762\tvalid_1's binary_logloss: 0.401452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400536:   0%|                  | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400536:  10%|#         | 1/10 [00:02<00:21,  2.43s/it]\u001b[32m[I 2023-02-13 22:54:11,365]\u001b[0m Trial 27 finished with value: 0.4005737736064209 and parameters: {'bagging_fraction': 0.7260429650751228, 'bagging_freq': 2}. Best is trial 27 with value: 0.4005737736064209.\u001b[0m\n",
      "bagging, val_score: 0.400536:  10%|#         | 1/10 [00:02<00:21,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's binary_logloss: 0.398948\tvalid_1's binary_logloss: 0.400574\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400536:  20%|##        | 2/10 [00:06<00:28,  3.57s/it]\u001b[32m[I 2023-02-13 22:54:15,820]\u001b[0m Trial 28 finished with value: 0.40058713197560464 and parameters: {'bagging_fraction': 0.6547105544499044, 'bagging_freq': 6}. Best is trial 27 with value: 0.4005737736064209.\u001b[0m\n",
      "bagging, val_score: 0.400536:  20%|##        | 2/10 [00:06<00:28,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's binary_logloss: 0.399191\tvalid_1's binary_logloss: 0.400587\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400536:  30%|###       | 3/10 [00:22<01:04,  9.28s/it]\u001b[32m[I 2023-02-13 22:54:31,805]\u001b[0m Trial 29 finished with value: 0.40066671496205614 and parameters: {'bagging_fraction': 0.4028313137145883, 'bagging_freq': 1}. Best is trial 27 with value: 0.4005737736064209.\u001b[0m\n",
      "bagging, val_score: 0.400536:  30%|###       | 3/10 [00:22<01:04,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398243\tvalid_1's binary_logloss: 0.400726\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's binary_logloss: 0.398338\tvalid_1's binary_logloss: 0.400667\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  40%|####      | 4/10 [00:26<00:42,  7.01s/it]\u001b[32m[I 2023-02-13 22:54:35,344]\u001b[0m Trial 30 finished with value: 0.4004801344354516 and parameters: {'bagging_fraction': 0.802449450836738, 'bagging_freq': 6}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  40%|####      | 4/10 [00:26<00:42,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398262\tvalid_1's binary_logloss: 0.400576\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.39841\tvalid_1's binary_logloss: 0.40048\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  50%|#####     | 5/10 [00:29<00:28,  5.62s/it]\u001b[32m[I 2023-02-13 22:54:38,491]\u001b[0m Trial 31 finished with value: 0.40069300625611554 and parameters: {'bagging_fraction': 0.4820239538111085, 'bagging_freq': 5}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  50%|#####     | 5/10 [00:29<00:28,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's binary_logloss: 0.39898\tvalid_1's binary_logloss: 0.400693\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  60%|######    | 6/10 [00:33<00:19,  4.96s/it]\u001b[32m[I 2023-02-13 22:54:42,172]\u001b[0m Trial 32 finished with value: 0.400598441846546 and parameters: {'bagging_fraction': 0.9347931725882498, 'bagging_freq': 2}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  60%|######    | 6/10 [00:33<00:19,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.398869\tvalid_1's binary_logloss: 0.400598\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  70%|#######   | 7/10 [00:35<00:12,  4.04s/it]\u001b[32m[I 2023-02-13 22:54:44,330]\u001b[0m Trial 33 finished with value: 0.4006252517177041 and parameters: {'bagging_fraction': 0.5111969317302304, 'bagging_freq': 1}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  70%|#######   | 7/10 [00:35<00:12,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's binary_logloss: 0.398912\tvalid_1's binary_logloss: 0.400625\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  80%|########  | 8/10 [00:37<00:06,  3.44s/it]\u001b[32m[I 2023-02-13 22:54:46,477]\u001b[0m Trial 34 finished with value: 0.40054595800362625 and parameters: {'bagging_fraction': 0.531818495575215, 'bagging_freq': 7}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  80%|########  | 8/10 [00:37<00:06,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.398735\tvalid_1's binary_logloss: 0.400546\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480:  90%|######### | 9/10 [00:40<00:03,  3.32s/it]\u001b[32m[I 2023-02-13 22:54:49,529]\u001b[0m Trial 35 finished with value: 0.4005394182693204 and parameters: {'bagging_fraction': 0.8870098894544057, 'bagging_freq': 2}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480:  90%|######### | 9/10 [00:40<00:03,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's binary_logloss: 0.39872\tvalid_1's binary_logloss: 0.400539\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.400480: 100%|#########| 10/10 [00:42<00:00,  2.97s/it]\u001b[32m[I 2023-02-13 22:54:51,728]\u001b[0m Trial 36 finished with value: 0.4005592533581782 and parameters: {'bagging_fraction': 0.8897348492363203, 'bagging_freq': 2}. Best is trial 30 with value: 0.4004801344354516.\u001b[0m\n",
      "bagging, val_score: 0.400480: 100%|#########| 10/10 [00:42<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.398733\tvalid_1's binary_logloss: 0.400559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400480:   0%|   | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400480:  17%|1| 1/6 [00:02<00:10,  2.0\u001b[32m[I 2023-02-13 22:54:53,755]\u001b[0m Trial 37 finished with value: 0.40049972874279666 and parameters: {'feature_fraction': 0.852}. Best is trial 37 with value: 0.40049972874279666.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400480:  17%|1| 1/6 [00:02<00:10,  2.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398319\tvalid_1's binary_logloss: 0.4005\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.398319\tvalid_1's binary_logloss: 0.4005\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400472:  33%|3| 2/6 [00:04<00:09,  2.2\u001b[32m[I 2023-02-13 22:54:56,201]\u001b[0m Trial 38 finished with value: 0.4004723469260217 and parameters: {'feature_fraction': 0.9799999999999999}. Best is trial 38 with value: 0.4004723469260217.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400472:  33%|3| 2/6 [00:04<00:09,  2.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400472:  50%|5| 3/6 [00:06<00:06,  2.2\u001b[32m[I 2023-02-13 22:54:58,334]\u001b[0m Trial 39 finished with value: 0.4004801344354516 and parameters: {'feature_fraction': 0.8839999999999999}. Best is trial 38 with value: 0.4004723469260217.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400472:  50%|5| 3/6 [00:06<00:06,  2.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398262\tvalid_1's binary_logloss: 0.400576\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.39841\tvalid_1's binary_logloss: 0.40048\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400472:  67%|6| 4/6 [00:09<00:04,  2.3\u001b[32m[I 2023-02-13 22:55:00,819]\u001b[0m Trial 40 finished with value: 0.4004801344354516 and parameters: {'feature_fraction': 0.9159999999999999}. Best is trial 38 with value: 0.4004723469260217.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400472:  67%|6| 4/6 [00:09<00:04,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398262\tvalid_1's binary_logloss: 0.400576\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.39841\tvalid_1's binary_logloss: 0.40048\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400472:  83%|8| 5/6 [00:11<00:02,  2.3\u001b[32m[I 2023-02-13 22:55:03,107]\u001b[0m Trial 41 finished with value: 0.40047927590449955 and parameters: {'feature_fraction': 0.82}. Best is trial 38 with value: 0.4004723469260217.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400472:  83%|8| 5/6 [00:11<00:02,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.398507\tvalid_1's binary_logloss: 0.400479\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.400472: 100%|#| 6/6 [00:14<00:00,  2.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.39866\tvalid_1's binary_logloss: 0.400487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-13 22:55:06,186]\u001b[0m Trial 42 finished with value: 0.4004865655993839 and parameters: {'feature_fraction': 0.948}. Best is trial 38 with value: 0.4004723469260217.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.400472: 100%|#| 6/6 [00:14<00:00,  2.4\n",
      "regularization_factors, val_score: 0.400472:   0%|   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:   5%| | 1/20 [00:02<00:44,  2.3\u001b[32m[I 2023-02-13 22:55:08,569]\u001b[0m Trial 43 finished with value: 0.40047233842024565 and parameters: {'lambda_l1': 0.0007773998922821829, 'lambda_l2': 3.2012859298995277e-06}. Best is trial 43 with value: 0.40047233842024565.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:   5%| | 1/20 [00:02<00:44,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  10%|1| 2/20 [00:05<00:46,  2.5\u001b[32m[I 2023-02-13 22:55:11,288]\u001b[0m Trial 44 finished with value: 0.40047586995886125 and parameters: {'lambda_l1': 6.616957066014342e-05, 'lambda_l2': 0.400853048601546}. Best is trial 43 with value: 0.40047233842024565.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  10%|1| 2/20 [00:05<00:46,  2.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.398502\tvalid_1's binary_logloss: 0.400476\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  15%|1| 3/20 [00:07<00:41,  2.4\u001b[32m[I 2023-02-13 22:55:13,543]\u001b[0m Trial 45 finished with value: 0.4004723469238754 and parameters: {'lambda_l1': 1.1027313099672533e-08, 'lambda_l2': 1.242001404761155e-07}. Best is trial 43 with value: 0.40047233842024565.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  15%|1| 3/20 [00:07<00:41,  2.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  20%|2| 4/20 [00:09<00:37,  2.3\u001b[32m[I 2023-02-13 22:55:15,829]\u001b[0m Trial 46 finished with value: 0.4005598079181634 and parameters: {'lambda_l1': 0.010882827930218712, 'lambda_l2': 0.2708162972907513}. Best is trial 43 with value: 0.40047233842024565.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  20%|2| 4/20 [00:09<00:37,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's binary_logloss: 0.398618\tvalid_1's binary_logloss: 0.40056\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  25%|2| 5/20 [00:11<00:35,  2.3\u001b[32m[I 2023-02-13 22:55:18,097]\u001b[0m Trial 47 finished with value: 0.4004723182064934 and parameters: {'lambda_l1': 1.6996492507894156e-07, 'lambda_l2': 0.0014991323116035308}. Best is trial 47 with value: 0.4004723182064934.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  25%|2| 5/20 [00:11<00:35,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400562\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398423\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  30%|3| 6/20 [00:14<00:32,  2.3\u001b[32m[I 2023-02-13 22:55:20,462]\u001b[0m Trial 48 finished with value: 0.40050560182451117 and parameters: {'lambda_l1': 1.0517138394360073, 'lambda_l2': 7.635176818135586e-07}. Best is trial 47 with value: 0.4004723182064934.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  30%|3| 6/20 [00:14<00:32,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398324\tvalid_1's binary_logloss: 0.400602\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398489\tvalid_1's binary_logloss: 0.400506\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400472:  35%|3| 7/20 [00:16<00:29,  2.3\u001b[32m[I 2023-02-13 22:55:22,657]\u001b[0m Trial 49 finished with value: 0.40047234691940825 and parameters: {'lambda_l1': 4.655367559816141e-07, 'lambda_l2': 9.449134137745608e-08}. Best is trial 47 with value: 0.4004723182064934.\u001b[0m\n",
      "regularization_factors, val_score: 0.400472:  35%|3| 7/20 [00:16<00:29,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  40%|4| 8/20 [00:19<00:29,  2.4\u001b[32m[I 2023-02-13 22:55:25,565]\u001b[0m Trial 50 finished with value: 0.4004427485061476 and parameters: {'lambda_l1': 9.490245203532942e-07, 'lambda_l2': 6.421168438428032}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  40%|4| 8/20 [00:19<00:29,  2.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398383\tvalid_1's binary_logloss: 0.400463\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.398507\tvalid_1's binary_logloss: 0.400443\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  45%|4| 9/20 [00:21<00:25,  2.3\u001b[32m[I 2023-02-13 22:55:27,612]\u001b[0m Trial 51 finished with value: 0.400481970908187 and parameters: {'lambda_l1': 0.2019055894080857, 'lambda_l2': 3.5275169933928286e-07}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  45%|4| 9/20 [00:21<00:25,  2.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's binary_logloss: 0.398577\tvalid_1's binary_logloss: 0.400482\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  50%|5| 10/20 [00:23<00:22,  2.\u001b[32m[I 2023-02-13 22:55:29,578]\u001b[0m Trial 52 finished with value: 0.4004832772456694 and parameters: {'lambda_l1': 0.22183125618514202, 'lambda_l2': 2.9286247167445133e-06}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  50%|5| 10/20 [00:23<00:22,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.39829\tvalid_1's binary_logloss: 0.400547\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's binary_logloss: 0.398472\tvalid_1's binary_logloss: 0.400483\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  55%|5| 11/20 [00:25<00:20,  2.\u001b[32m[I 2023-02-13 22:55:31,872]\u001b[0m Trial 53 finished with value: 0.400510576546946 and parameters: {'lambda_l1': 3.005784364160781e-05, 'lambda_l2': 8.38297710342227}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  55%|5| 11/20 [00:25<00:20,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.398702\tvalid_1's binary_logloss: 0.400511\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  60%|6| 12/20 [00:28<00:18,  2.\u001b[32m[I 2023-02-13 22:55:34,345]\u001b[0m Trial 54 finished with value: 0.4004723401554088 and parameters: {'lambda_l1': 1.187191828926573e-06, 'lambda_l2': 0.00035256149866694594}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  60%|6| 12/20 [00:28<00:18,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400562\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398423\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  65%|6| 13/20 [00:30<00:16,  2.\u001b[32m[I 2023-02-13 22:55:36,709]\u001b[0m Trial 55 finished with value: 0.4004723009083789 and parameters: {'lambda_l1': 1.5326118358532145e-08, 'lambda_l2': 0.0024056973309017414}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  65%|6| 13/20 [00:30<00:16,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400562\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398423\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  70%|7| 14/20 [00:33<00:14,  2.\u001b[32m[I 2023-02-13 22:55:39,289]\u001b[0m Trial 56 finished with value: 0.40046463038258134 and parameters: {'lambda_l1': 1.8773006530383478e-08, 'lambda_l2': 0.0053577653024348805}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  70%|7| 14/20 [00:33<00:14,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398262\tvalid_1's binary_logloss: 0.400569\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398434\tvalid_1's binary_logloss: 0.400465\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  75%|7| 15/20 [00:35<00:11,  2.\u001b[32m[I 2023-02-13 22:55:41,220]\u001b[0m Trial 57 finished with value: 0.4004738025533113 and parameters: {'lambda_l1': 4.671929389608935e-06, 'lambda_l2': 0.031213977783275056}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  75%|7| 15/20 [00:35<00:11,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398252\tvalid_1's binary_logloss: 0.400496\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's binary_logloss: 0.398382\tvalid_1's binary_logloss: 0.400474\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  80%|8| 16/20 [00:37<00:08,  2.\u001b[32m[I 2023-02-13 22:55:43,271]\u001b[0m Trial 58 finished with value: 0.4004723459633982 and parameters: {'lambda_l1': 9.888647633264006e-08, 'lambda_l2': 5.002236173419203e-05}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  80%|8| 16/20 [00:37<00:08,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  85%|8| 17/20 [00:39<00:06,  2.\u001b[32m[I 2023-02-13 22:55:45,260]\u001b[0m Trial 59 finished with value: 0.4005243381784541 and parameters: {'lambda_l1': 1.548093116837356e-06, 'lambda_l2': 8.883683170189627}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  85%|8| 17/20 [00:39<00:06,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's binary_logloss: 0.398676\tvalid_1's binary_logloss: 0.400524\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  90%|9| 18/20 [00:41<00:04,  2.\u001b[32m[I 2023-02-13 22:55:47,209]\u001b[0m Trial 60 finished with value: 0.4004740763827764 and parameters: {'lambda_l1': 1.0097469443659385e-07, 'lambda_l2': 0.020503066366016755}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  90%|9| 18/20 [00:41<00:04,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398249\tvalid_1's binary_logloss: 0.400497\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's binary_logloss: 0.39838\tvalid_1's binary_logloss: 0.400474\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443:  95%|9| 19/20 [00:43<00:02,  2.\u001b[32m[I 2023-02-13 22:55:49,316]\u001b[0m Trial 61 finished with value: 0.40047234683002986 and parameters: {'lambda_l1': 8.921249493135996e-06, 'lambda_l2': 1.505425473668919e-08}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443:  95%|9| 19/20 [00:43<00:02,  2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398247\tvalid_1's binary_logloss: 0.400563\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398422\tvalid_1's binary_logloss: 0.400472\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.398306\tvalid_1's binary_logloss: 0.400524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.400443: 100%|#| 20/20 [00:45<00:00,  2.\u001b[32m[I 2023-02-13 22:55:51,842]\u001b[0m Trial 62 finished with value: 0.400494819862985 and parameters: {'lambda_l1': 0.0002663058340698051, 'lambda_l2': 0.7995310882728119}. Best is trial 50 with value: 0.4004427485061476.\u001b[0m\n",
      "regularization_factors, val_score: 0.400443: 100%|#| 20/20 [00:45<00:00,  2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.397835\tvalid_1's binary_logloss: 0.400495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400443:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400443:  20%|4 | 1/5 [00:02<00:08,  2.00s/it]\u001b[32m[I 2023-02-13 22:55:53,856]\u001b[0m Trial 63 finished with value: 0.4004427485061476 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.4004427485061476.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.400443:  20%|4 | 1/5 [00:02<00:08,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398383\tvalid_1's binary_logloss: 0.400463\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.398507\tvalid_1's binary_logloss: 0.400443\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400434:  40%|8 | 2/5 [00:03<00:05,  1.97s/it]\u001b[32m[I 2023-02-13 22:55:55,797]\u001b[0m Trial 64 finished with value: 0.40043437039415675 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.40043437039415675.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.400434:  40%|8 | 2/5 [00:03<00:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's binary_logloss: 0.398543\tvalid_1's binary_logloss: 0.400434\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400434:  60%|#2| 3/5 [00:06<00:04,  2.03s/it]\u001b[32m[I 2023-02-13 22:55:57,901]\u001b[0m Trial 65 finished with value: 0.4004427485061476 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.40043437039415675.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.400434:  60%|#2| 3/5 [00:06<00:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398383\tvalid_1's binary_logloss: 0.400463\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.398507\tvalid_1's binary_logloss: 0.400443\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400434:  80%|#6| 4/5 [00:09<00:02,  2.56s/it]\u001b[32m[I 2023-02-13 22:56:01,265]\u001b[0m Trial 66 finished with value: 0.4004427485061476 and parameters: {'min_child_samples': 5}. Best is trial 64 with value: 0.40043437039415675.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.400434:  80%|#6| 4/5 [00:09<00:02,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398383\tvalid_1's binary_logloss: 0.400463\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.398507\tvalid_1's binary_logloss: 0.400443\n",
      "[LightGBM] [Info] Number of positive: 66626, number of negative: 251914\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1534\n",
      "[LightGBM] [Info] Number of data points in the train set: 318540, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209161 -> initscore=-1.329993\n",
      "[LightGBM] [Info] Start training from score -1.329993\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.400426: 100%|##| 5/5 [00:11<00:00,  2.37s/it]\u001b[32m[I 2023-02-13 22:56:03,298]\u001b[0m Trial 67 finished with value: 0.40042619216644565 and parameters: {'min_child_samples': 100}. Best is trial 67 with value: 0.40042619216644565.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.400426: 100%|##| 5/5 [00:11<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.398379\tvalid_1's binary_logloss: 0.400481\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.398513\tvalid_1's binary_logloss: 0.400426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7c758187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 9.490245203532942e-07,\n",
       " 'lambda_l2': 6.421168438428032,\n",
       " 'num_leaves': 9,\n",
       " 'feature_fraction': 0.9799999999999999,\n",
       " 'bagging_fraction': 0.802449450836738,\n",
       " 'bagging_freq': 6,\n",
       " 'min_child_samples': 100,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "attachments": {
    "%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202023-02-10%2018.33.35.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0e69c05d",
   "metadata": {},
   "source": [
    "![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202023-02-10%2018.33.35.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202023-02-10%2018.33.35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5152e7b",
   "metadata": {},
   "source": [
    "# LightGBMで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6c8762b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9799999999999999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9799999999999999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.802449450836738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.802449450836738\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.421168438428032, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.421168438428032\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.490245203532942e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.490245203532942e-07\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.802449450836738, bagging_freq=6,\n",
       "               feature_fraction=0.9799999999999999, feature_pre_filter=False,\n",
       "               lambda_l1=9.490245203532942e-07, lambda_l2=6.421168438428032,\n",
       "               min_child_samples=100, num_iterations=1000, num_leaves=9,\n",
       "               objective=&#x27;binary&#x27;, random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(bagging_fraction=0.802449450836738, bagging_freq=6,\n",
       "               feature_fraction=0.9799999999999999, feature_pre_filter=False,\n",
       "               lambda_l1=9.490245203532942e-07, lambda_l2=6.421168438428032,\n",
       "               min_child_samples=100, num_iterations=1000, num_leaves=9,\n",
       "               objective=&#x27;binary&#x27;, random_state=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.802449450836738, bagging_freq=6,\n",
       "               feature_fraction=0.9799999999999999, feature_pre_filter=False,\n",
       "               lambda_l1=9.490245203532942e-07, lambda_l2=6.421168438428032,\n",
       "               min_child_samples=100, num_iterations=1000, num_leaves=9,\n",
       "               objective='binary', random_state=100)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証データも含めるために元のデータから分割\n",
    "train, test = split_data(df)\n",
    "\n",
    "# X_train = train.drop(['rank', 'date', '単勝'], axis=1)\n",
    "X_train = train.drop(['rank', 'date'], axis=1)\n",
    "y_train = train['rank']\n",
    "\n",
    "X_test = test.drop(['rank', 'date'], axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "params = lgb_clf_o.params.copy()\n",
    "del(params['early_stopping_round'])\n",
    "\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c464f4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>course_len</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>齢</th>\n",
       "      <th>体重</th>\n",
       "      <th>体重変化</th>\n",
       "      <th>開催</th>\n",
       "      <th>...</th>\n",
       "      <th>race_type_芝</th>\n",
       "      <th>race_type_ダート</th>\n",
       "      <th>race_type_障害</th>\n",
       "      <th>ground_state_良</th>\n",
       "      <th>ground_state_稍重</th>\n",
       "      <th>ground_state_不良</th>\n",
       "      <th>ground_state_重</th>\n",
       "      <th>性_牡</th>\n",
       "      <th>性_牝</th>\n",
       "      <th>性_セ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2005106677</td>\n",
       "      <td>422</td>\n",
       "      <td>5</td>\n",
       "      <td>520.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2005103849</td>\n",
       "      <td>1065</td>\n",
       "      <td>5</td>\n",
       "      <td>498.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2006102986</td>\n",
       "      <td>641</td>\n",
       "      <td>4</td>\n",
       "      <td>504.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2004110041</td>\n",
       "      <td>733</td>\n",
       "      <td>6</td>\n",
       "      <td>528.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201006010108</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2004102881</td>\n",
       "      <td>1096</td>\n",
       "      <td>6</td>\n",
       "      <td>520.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010408</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016105169</td>\n",
       "      <td>5203</td>\n",
       "      <td>3</td>\n",
       "      <td>456.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010408</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016104841</td>\n",
       "      <td>1088</td>\n",
       "      <td>3</td>\n",
       "      <td>466.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010408</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016101679</td>\n",
       "      <td>5115</td>\n",
       "      <td>3</td>\n",
       "      <td>426.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010408</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016104400</td>\n",
       "      <td>1102</td>\n",
       "      <td>3</td>\n",
       "      <td>460.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201909010408</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>54.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016100978</td>\n",
       "      <td>1133</td>\n",
       "      <td>3</td>\n",
       "      <td>404.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452359 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              枠番  馬番    斤量  course_len    horse_id  jockey_id  齢     体重  体重変化  \\\n",
       "201006010108   7  12  57.0        18.0  2005106677        422  5  520.0   6.0   \n",
       "201006010108   3   4  57.0        18.0  2005103849       1065  5  498.0   6.0   \n",
       "201006010108   4   6  56.0        18.0  2006102986        641  4  504.0  -8.0   \n",
       "201006010108   4   5  55.0        18.0  2004110041        733  6  528.0  -2.0   \n",
       "201006010108   6   9  57.0        18.0  2004102881       1096  6  520.0  -2.0   \n",
       "...           ..  ..   ...         ...         ...        ... ..    ...   ...   \n",
       "201909010408   6   6  56.0        18.0  2016105169       5203  3  456.0   6.0   \n",
       "201909010408   4   4  56.0        18.0  2016104841       1088  3  466.0   8.0   \n",
       "201909010408   1   1  54.0        18.0  2016101679       5115  3  426.0  -8.0   \n",
       "201909010408   7   7  56.0        18.0  2016104400       1102  3  460.0  -2.0   \n",
       "201909010408   3   3  54.0        18.0  2016100978       1133  3  404.0 -10.0   \n",
       "\n",
       "              開催  ...  race_type_芝  race_type_ダート  race_type_障害  \\\n",
       "201006010108   6  ...            0              1             0   \n",
       "201006010108   6  ...            0              1             0   \n",
       "201006010108   6  ...            0              1             0   \n",
       "201006010108   6  ...            0              1             0   \n",
       "201006010108   6  ...            0              1             0   \n",
       "...           ..  ...          ...            ...           ...   \n",
       "201909010408   9  ...            1              0             0   \n",
       "201909010408   9  ...            1              0             0   \n",
       "201909010408   9  ...            1              0             0   \n",
       "201909010408   9  ...            1              0             0   \n",
       "201909010408   9  ...            1              0             0   \n",
       "\n",
       "              ground_state_良  ground_state_稍重  ground_state_不良  \\\n",
       "201006010108               1                0                0   \n",
       "201006010108               1                0                0   \n",
       "201006010108               1                0                0   \n",
       "201006010108               1                0                0   \n",
       "201006010108               1                0                0   \n",
       "...                      ...              ...              ...   \n",
       "201909010408               1                0                0   \n",
       "201909010408               1                0                0   \n",
       "201909010408               1                0                0   \n",
       "201909010408               1                0                0   \n",
       "201909010408               1                0                0   \n",
       "\n",
       "              ground_state_重  性_牡  性_牝  性_セ  \n",
       "201006010108               0    1    0    0  \n",
       "201006010108               0    1    0    0  \n",
       "201006010108               0    1    0    0  \n",
       "201006010108               0    0    1    0  \n",
       "201006010108               0    0    0    1  \n",
       "...                      ...  ...  ...  ...  \n",
       "201909010408               0    1    0    0  \n",
       "201909010408               0    1    0    0  \n",
       "201909010408               0    0    1    0  \n",
       "201909010408               0    1    0    0  \n",
       "201909010408               0    0    1    0  \n",
       "\n",
       "[452359 rows x 29 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864ec89",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ba45f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = lgb_clf.predict(X_test.drop(['単勝'], axis=1).values)\n",
    "# y_predict_proba = lgb_clf.predict_proba(X_test.drop(['単勝'], axis=1).values)\n",
    "y_predict = lgb_clf.predict(X_test.values)\n",
    "y_predict_proba = lgb_clf.predict_proba(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8399ae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('単勝', 1323),\n",
       " ('jockey_id', 1101),\n",
       " ('horse_id', 1064),\n",
       " ('体重', 952),\n",
       " ('体重変化', 566),\n",
       " ('馬番', 459),\n",
       " ('course_len', 378),\n",
       " ('開催', 376),\n",
       " ('n_horses', 367),\n",
       " ('斤量', 287)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_dict = dict()\n",
    "\n",
    "for c, i in zip(X_train.columns, lgb_clf.feature_importances_):\n",
    "    feature_importance_dict[c] = i\n",
    "\n",
    "feature_importance_dict = sorted(feature_importance_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "feature_importance_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "475e6b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "684a6cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18583839, 0.48843801, 0.21527116, ..., 0.20159589, 0.07930137,\n",
       "       0.1248944 ])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ce996de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8100832442842594"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test.values, y_predict_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5e62b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = 'model.pkl'\n",
    "pickle.dump(lgb_clf, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "82e57c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452359 entries, 201006010108 to 201909010408\n",
      "Data columns (total 30 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   枠番               452359 non-null  int64  \n",
      " 1   馬番               452359 non-null  int64  \n",
      " 2   斤量               452359 non-null  float64\n",
      " 3   単勝               452359 non-null  float64\n",
      " 4   course_len       452359 non-null  float64\n",
      " 5   horse_id         452359 non-null  int64  \n",
      " 6   jockey_id        452359 non-null  int64  \n",
      " 7   齢                452359 non-null  int64  \n",
      " 8   体重               452358 non-null  float64\n",
      " 9   体重変化             452358 non-null  float64\n",
      " 10  開催               452359 non-null  int64  \n",
      " 11  n_horses         452359 non-null  int64  \n",
      " 12  jockey_label     452359 non-null  int64  \n",
      " 13  horse_label      452359 non-null  int64  \n",
      " 14  weather_晴        452359 non-null  int64  \n",
      " 15  weather_曇        452359 non-null  int64  \n",
      " 16  weather_小雨       452359 non-null  int64  \n",
      " 17  weather_雨        452359 non-null  int64  \n",
      " 18  weather_小雪       452359 non-null  int64  \n",
      " 19  weather_雪        452359 non-null  int64  \n",
      " 20  race_type_芝      452359 non-null  int64  \n",
      " 21  race_type_ダート    452359 non-null  int64  \n",
      " 22  race_type_障害     452359 non-null  int64  \n",
      " 23  ground_state_良   452359 non-null  int64  \n",
      " 24  ground_state_稍重  452359 non-null  int64  \n",
      " 25  ground_state_不良  452359 non-null  int64  \n",
      " 26  ground_state_重   452359 non-null  int64  \n",
      " 27  性_牡              452359 non-null  int64  \n",
      " 28  性_牝              452359 non-null  int64  \n",
      " 29  性_セ              452359 non-null  int64  \n",
      "dtypes: float64(5), int64(25)\n",
      "memory usage: 107.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf24f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e1e0c01",
   "metadata": {},
   "source": [
    "# 最終モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c98ab7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-14 00:45:19,887]\u001b[0m A new study created in memory with name: no-name-8f9a7dcd-6138-4838-b949-86f2825f3091\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|               | 0/7 [00:00<?, ?it/s]/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.411078:  14%|2 | 1/7 [00:02<00:17,  2.94s/it]\u001b[32m[I 2023-02-14 00:45:22,843]\u001b[0m Trial 0 finished with value: 0.4110775495882078 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.4110775495882078.\u001b[0m\n",
      "feature_fraction, val_score: 0.411078:  14%|2 | 1/7 [00:02<00:17,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.397388\tvalid_1's binary_logloss: 0.411078\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410946:  29%|5 | 2/7 [00:05<00:13,  2.65s/it]\u001b[32m[I 2023-02-14 00:45:25,288]\u001b[0m Trial 1 finished with value: 0.41094581978849726 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.41094581978849726.\u001b[0m\n",
      "feature_fraction, val_score: 0.410946:  29%|5 | 2/7 [00:05<00:13,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's binary_logloss: 0.397013\tvalid_1's binary_logloss: 0.410946\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410945:  43%|8 | 3/7 [00:07<00:09,  2.46s/it]\u001b[32m[I 2023-02-14 00:45:27,521]\u001b[0m Trial 2 finished with value: 0.41094545270265065 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 0.41094545270265065.\u001b[0m\n",
      "feature_fraction, val_score: 0.410945:  43%|8 | 3/7 [00:07<00:09,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.398857\tvalid_1's binary_logloss: 0.410945\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410945:  57%|#1| 4/7 [00:09<00:07,  2.41s/it]\u001b[32m[I 2023-02-14 00:45:29,855]\u001b[0m Trial 3 finished with value: 0.4110284681475017 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 2 with value: 0.41094545270265065.\u001b[0m\n",
      "feature_fraction, val_score: 0.410945:  57%|#1| 4/7 [00:09<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.398609\tvalid_1's binary_logloss: 0.411028\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410902:  71%|#4| 5/7 [00:12<00:04,  2.42s/it]\u001b[32m[I 2023-02-14 00:45:32,297]\u001b[0m Trial 4 finished with value: 0.41090226048037154 and parameters: {'feature_fraction': 0.6}. Best is trial 4 with value: 0.41090226048037154.\u001b[0m\n",
      "feature_fraction, val_score: 0.410902:  71%|#4| 5/7 [00:12<00:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.398274\tvalid_1's binary_logloss: 0.410902\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410902:  86%|#7| 6/7 [00:15<00:02,  2.50s/it]\u001b[32m[I 2023-02-14 00:45:34,962]\u001b[0m Trial 5 finished with value: 0.4109097469417117 and parameters: {'feature_fraction': 1.0}. Best is trial 4 with value: 0.41090226048037154.\u001b[0m\n",
      "feature_fraction, val_score: 0.410902:  86%|#7| 6/7 [00:15<00:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's binary_logloss: 0.397629\tvalid_1's binary_logloss: 0.41091\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.410902: 100%|##| 7/7 [00:17<00:00,  2.40s/it]\u001b[32m[I 2023-02-14 00:45:37,152]\u001b[0m Trial 6 finished with value: 0.4110734057797623 and parameters: {'feature_fraction': 0.7}. Best is trial 4 with value: 0.41090226048037154.\u001b[0m\n",
      "feature_fraction, val_score: 0.410902: 100%|##| 7/7 [00:17<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.398252\tvalid_1's binary_logloss: 0.411073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410902:   0%|               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410902:   5%|3      | 1/20 [00:02<00:47,  2.51s/it]\u001b[32m[I 2023-02-14 00:45:39,681]\u001b[0m Trial 7 finished with value: 0.41249350532215695 and parameters: {'num_leaves': 140}. Best is trial 7 with value: 0.41249350532215695.\u001b[0m\n",
      "num_leaves, val_score: 0.410902:   5%|3      | 1/20 [00:02<00:47,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's binary_logloss: 0.391425\tvalid_1's binary_logloss: 0.412494\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410902:  10%|7      | 2/20 [00:04<00:44,  2.49s/it]\u001b[32m[I 2023-02-14 00:45:42,149]\u001b[0m Trial 8 finished with value: 0.4116800343659208 and parameters: {'num_leaves': 72}. Best is trial 8 with value: 0.4116800343659208.\u001b[0m\n",
      "num_leaves, val_score: 0.410902:  10%|7      | 2/20 [00:04<00:44,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.394165\tvalid_1's binary_logloss: 0.41168\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410902:  15%|#      | 3/20 [00:07<00:42,  2.49s/it]\u001b[32m[I 2023-02-14 00:45:44,648]\u001b[0m Trial 9 finished with value: 0.41212840286115143 and parameters: {'num_leaves': 110}. Best is trial 8 with value: 0.4116800343659208.\u001b[0m\n",
      "num_leaves, val_score: 0.410902:  15%|#      | 3/20 [00:07<00:42,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.394359\tvalid_1's binary_logloss: 0.412128\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410902:  20%|#4     | 4/20 [00:10<00:42,  2.66s/it]\u001b[32m[I 2023-02-14 00:45:47,570]\u001b[0m Trial 10 finished with value: 0.41316552949296226 and parameters: {'num_leaves': 217}. Best is trial 8 with value: 0.4116800343659208.\u001b[0m\n",
      "num_leaves, val_score: 0.410902:  20%|#4     | 4/20 [00:10<00:42,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.386469\tvalid_1's binary_logloss: 0.413166\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399929\tvalid_1's binary_logloss: 0.4104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  25%|#7     | 5/20 [00:13<00:39,  2.67s/it]\u001b[32m[I 2023-02-14 00:45:50,240]\u001b[0m Trial 11 finished with value: 0.4102357356022424 and parameters: {'num_leaves': 3}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  25%|#7     | 5/20 [00:13<00:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[159]\tvalid_0's binary_logloss: 0.399669\tvalid_1's binary_logloss: 0.410236\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  30%|##1    | 6/20 [00:15<00:36,  2.58s/it]\u001b[32m[I 2023-02-14 00:45:52,645]\u001b[0m Trial 12 finished with value: 0.41104307326330103 and parameters: {'num_leaves': 33}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  30%|##1    | 6/20 [00:15<00:36,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.39759\tvalid_1's binary_logloss: 0.411043\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  35%|##4    | 7/20 [00:17<00:33,  2.55s/it]\u001b[32m[I 2023-02-14 00:45:55,126]\u001b[0m Trial 13 finished with value: 0.41285287756275235 and parameters: {'num_leaves': 173}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  35%|##4    | 7/20 [00:17<00:33,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's binary_logloss: 0.390473\tvalid_1's binary_logloss: 0.412853\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  40%|##8    | 8/20 [00:20<00:31,  2.60s/it]\u001b[32m[I 2023-02-14 00:45:57,835]\u001b[0m Trial 14 finished with value: 0.41312559896482043 and parameters: {'num_leaves': 212}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  40%|##8    | 8/20 [00:20<00:31,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's binary_logloss: 0.387079\tvalid_1's binary_logloss: 0.413126\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  45%|###1   | 9/20 [00:23<00:28,  2.62s/it]\u001b[32m[I 2023-02-14 00:46:00,503]\u001b[0m Trial 15 finished with value: 0.41120734307964835 and parameters: {'num_leaves': 36}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  45%|###1   | 9/20 [00:23<00:28,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's binary_logloss: 0.3977\tvalid_1's binary_logloss: 0.411207\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  50%|###   | 10/20 [00:25<00:26,  2.60s/it]\u001b[32m[I 2023-02-14 00:46:03,061]\u001b[0m Trial 16 finished with value: 0.4122970480614404 and parameters: {'num_leaves': 148}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  50%|###   | 10/20 [00:25<00:26,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's binary_logloss: 0.390952\tvalid_1's binary_logloss: 0.412297\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.400962\tvalid_1's binary_logloss: 0.411323\n",
      "[200]\tvalid_0's binary_logloss: 0.399944\tvalid_1's binary_logloss: 0.41045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  55%|###3  | 11/20 [00:29<00:26,  2.90s/it]\u001b[32m[I 2023-02-14 00:46:06,635]\u001b[0m Trial 17 finished with value: 0.4103996475485706 and parameters: {'num_leaves': 2}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  55%|###3  | 11/20 [00:29<00:26,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[267]\tvalid_0's binary_logloss: 0.399823\tvalid_1's binary_logloss: 0.4104\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  60%|###6  | 12/20 [00:31<00:21,  2.69s/it]\u001b[32m[I 2023-02-14 00:46:08,852]\u001b[0m Trial 18 finished with value: 0.41030688606817745 and parameters: {'num_leaves': 9}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  60%|###6  | 12/20 [00:31<00:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's binary_logloss: 0.39936\tvalid_1's binary_logloss: 0.410307\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.400962\tvalid_1's binary_logloss: 0.411323\n",
      "[200]\tvalid_0's binary_logloss: 0.399944\tvalid_1's binary_logloss: 0.41045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  65%|###9  | 13/20 [00:35<00:20,  2.96s/it]\u001b[32m[I 2023-02-14 00:46:12,425]\u001b[0m Trial 19 finished with value: 0.4103996475485706 and parameters: {'num_leaves': 2}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  65%|###9  | 13/20 [00:35<00:20,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[267]\tvalid_0's binary_logloss: 0.399823\tvalid_1's binary_logloss: 0.4104\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  70%|####1 | 14/20 [00:37<00:16,  2.83s/it]\u001b[32m[I 2023-02-14 00:46:14,952]\u001b[0m Trial 20 finished with value: 0.41175369346165286 and parameters: {'num_leaves': 77}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  70%|####1 | 14/20 [00:37<00:16,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.394628\tvalid_1's binary_logloss: 0.411754\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  75%|####5 | 15/20 [00:40<00:13,  2.69s/it]\u001b[32m[I 2023-02-14 00:46:17,308]\u001b[0m Trial 21 finished with value: 0.4115047394199577 and parameters: {'num_leaves': 51}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  75%|####5 | 15/20 [00:40<00:13,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.396719\tvalid_1's binary_logloss: 0.411505\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  80%|####8 | 16/20 [00:42<00:10,  2.71s/it]\u001b[32m[I 2023-02-14 00:46:20,063]\u001b[0m Trial 22 finished with value: 0.4136592029237828 and parameters: {'num_leaves': 254}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  80%|####8 | 16/20 [00:42<00:10,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.387807\tvalid_1's binary_logloss: 0.413659\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  85%|#####1| 17/20 [00:45<00:07,  2.62s/it]\u001b[32m[I 2023-02-14 00:46:22,493]\u001b[0m Trial 23 finished with value: 0.4118516672041244 and parameters: {'num_leaves': 85}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  85%|#####1| 17/20 [00:45<00:07,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.394761\tvalid_1's binary_logloss: 0.411852\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  90%|#####4| 18/20 [00:47<00:04,  2.47s/it]\u001b[32m[I 2023-02-14 00:46:24,617]\u001b[0m Trial 24 finished with value: 0.41075568713941035 and parameters: {'num_leaves': 24}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  90%|#####4| 18/20 [00:47<00:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.39871\tvalid_1's binary_logloss: 0.410756\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236:  95%|#####6| 19/20 [00:49<00:02,  2.50s/it]\u001b[32m[I 2023-02-14 00:46:27,166]\u001b[0m Trial 25 finished with value: 0.41202356927204203 and parameters: {'num_leaves': 98}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236:  95%|#####6| 19/20 [00:50<00:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's binary_logloss: 0.39405\tvalid_1's binary_logloss: 0.412024\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.410236: 100%|######| 20/20 [00:53<00:00,  2.73s/it]\u001b[32m[I 2023-02-14 00:46:30,440]\u001b[0m Trial 26 finished with value: 0.41146045456174035 and parameters: {'num_leaves': 57}. Best is trial 11 with value: 0.4102357356022424.\u001b[0m\n",
      "num_leaves, val_score: 0.410236: 100%|######| 20/20 [00:53<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.396298\tvalid_1's binary_logloss: 0.41146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:   0%|                  | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  10%|#         | 1/10 [00:03<00:34,  3.84s/it]\u001b[32m[I 2023-02-14 00:46:34,299]\u001b[0m Trial 27 finished with value: 0.4104067542795759 and parameters: {'bagging_fraction': 0.7260429650751228, 'bagging_freq': 2}. Best is trial 27 with value: 0.4104067542795759.\u001b[0m\n",
      "bagging, val_score: 0.410236:  10%|#         | 1/10 [00:03<00:34,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.399932\tvalid_1's binary_logloss: 0.41049\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.399993\tvalid_1's binary_logloss: 0.410407\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  20%|##        | 2/10 [00:08<00:34,  4.35s/it]\u001b[32m[I 2023-02-14 00:46:39,001]\u001b[0m Trial 28 finished with value: 0.4103412122248295 and parameters: {'bagging_fraction': 0.6547105544499044, 'bagging_freq': 6}. Best is trial 28 with value: 0.4103412122248295.\u001b[0m\n",
      "bagging, val_score: 0.410236:  20%|##        | 2/10 [00:08<00:34,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[142]\tvalid_0's binary_logloss: 0.399725\tvalid_1's binary_logloss: 0.410341\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399907\tvalid_1's binary_logloss: 0.410423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  30%|###       | 3/10 [00:11<00:27,  3.86s/it]\u001b[32m[I 2023-02-14 00:46:42,289]\u001b[0m Trial 29 finished with value: 0.4103948575929634 and parameters: {'bagging_fraction': 0.4028313137145883, 'bagging_freq': 1}. Best is trial 28 with value: 0.4103412122248295.\u001b[0m\n",
      "bagging, val_score: 0.410236:  30%|###       | 3/10 [00:11<00:27,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's binary_logloss: 0.399873\tvalid_1's binary_logloss: 0.410395\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399941\tvalid_1's binary_logloss: 0.410419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  40%|####      | 4/10 [00:15<00:22,  3.73s/it]\u001b[32m[I 2023-02-14 00:46:45,805]\u001b[0m Trial 30 finished with value: 0.4102433525173247 and parameters: {'bagging_fraction': 0.802449450836738, 'bagging_freq': 6}. Best is trial 30 with value: 0.4102433525173247.\u001b[0m\n",
      "bagging, val_score: 0.410236:  40%|####      | 4/10 [00:15<00:22,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's binary_logloss: 0.399708\tvalid_1's binary_logloss: 0.410243\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399934\tvalid_1's binary_logloss: 0.41041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  50%|#####     | 5/10 [00:17<00:16,  3.32s/it]\u001b[32m[I 2023-02-14 00:46:48,394]\u001b[0m Trial 31 finished with value: 0.41029240949269324 and parameters: {'bagging_fraction': 0.4820239538111085, 'bagging_freq': 5}. Best is trial 30 with value: 0.4102433525173247.\u001b[0m\n",
      "bagging, val_score: 0.410236:  50%|#####     | 5/10 [00:17<00:16,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.399753\tvalid_1's binary_logloss: 0.410292\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399928\tvalid_1's binary_logloss: 0.410386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  60%|######    | 6/10 [00:20<00:12,  3.23s/it]\u001b[32m[I 2023-02-14 00:46:51,457]\u001b[0m Trial 32 finished with value: 0.4103162259378431 and parameters: {'bagging_fraction': 0.9347931725882498, 'bagging_freq': 2}. Best is trial 30 with value: 0.4102433525173247.\u001b[0m\n",
      "bagging, val_score: 0.410236:  60%|######    | 6/10 [00:21<00:12,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's binary_logloss: 0.399808\tvalid_1's binary_logloss: 0.410316\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399918\tvalid_1's binary_logloss: 0.410448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  70%|#######   | 7/10 [00:23<00:09,  3.12s/it]\u001b[32m[I 2023-02-14 00:46:54,353]\u001b[0m Trial 33 finished with value: 0.41034709960650645 and parameters: {'bagging_fraction': 0.5111969317302304, 'bagging_freq': 1}. Best is trial 30 with value: 0.4102433525173247.\u001b[0m\n",
      "bagging, val_score: 0.410236:  70%|#######   | 7/10 [00:23<00:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[133]\tvalid_0's binary_logloss: 0.399736\tvalid_1's binary_logloss: 0.410347\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410236:  80%|########  | 8/10 [00:26<00:05,  2.91s/it]\u001b[32m[I 2023-02-14 00:46:56,798]\u001b[0m Trial 34 finished with value: 0.41053251513334793 and parameters: {'bagging_fraction': 0.531818495575215, 'bagging_freq': 7}. Best is trial 30 with value: 0.4102433525173247.\u001b[0m\n",
      "bagging, val_score: 0.410236:  80%|########  | 8/10 [00:26<00:05,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.399945\tvalid_1's binary_logloss: 0.410618\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's binary_logloss: 0.399995\tvalid_1's binary_logloss: 0.410533\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39992\tvalid_1's binary_logloss: 0.410386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410211:  90%|######### | 9/10 [00:30<00:03,  3.38s/it]\u001b[32m[I 2023-02-14 00:47:01,208]\u001b[0m Trial 35 finished with value: 0.4102114118981729 and parameters: {'bagging_fraction': 0.8870098894544057, 'bagging_freq': 2}. Best is trial 35 with value: 0.4102114118981729.\u001b[0m\n",
      "bagging, val_score: 0.410211:  90%|######### | 9/10 [00:30<00:03,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399612\tvalid_1's binary_logloss: 0.410211\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399921\tvalid_1's binary_logloss: 0.410395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.410211: 100%|#########| 10/10 [00:34<00:00,  3.52s/it]\u001b[32m[I 2023-02-14 00:47:05,044]\u001b[0m Trial 36 finished with value: 0.41024767960563424 and parameters: {'bagging_fraction': 0.8897348492363203, 'bagging_freq': 2}. Best is trial 35 with value: 0.4102114118981729.\u001b[0m\n",
      "bagging, val_score: 0.410211: 100%|#########| 10/10 [00:34<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's binary_logloss: 0.399689\tvalid_1's binary_logloss: 0.410248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410211:   0%|   | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204:  17%|1| 1/6 [00:03<00:19,  3.9\u001b[32m[I 2023-02-14 00:47:09,004]\u001b[0m Trial 37 finished with value: 0.41020431590965467 and parameters: {'feature_fraction': 0.552}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204:  17%|1| 1/6 [00:03<00:19,  3.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39992\tvalid_1's binary_logloss: 0.410386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204:  33%|3| 2/6 [00:07<00:15,  3.9\u001b[32m[I 2023-02-14 00:47:12,868]\u001b[0m Trial 38 finished with value: 0.4102114118981729 and parameters: {'feature_fraction': 0.616}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204:  33%|3| 2/6 [00:07<00:15,  3.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399612\tvalid_1's binary_logloss: 0.410211\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39992\tvalid_1's binary_logloss: 0.410386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204:  50%|5| 3/6 [00:12<00:12,  4.0\u001b[32m[I 2023-02-14 00:47:17,084]\u001b[0m Trial 39 finished with value: 0.4102114118981729 and parameters: {'feature_fraction': 0.584}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204:  50%|5| 3/6 [00:12<00:12,  4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399612\tvalid_1's binary_logloss: 0.410211\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399925\tvalid_1's binary_logloss: 0.410333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204:  67%|6| 4/6 [00:15<00:07,  3.9\u001b[32m[I 2023-02-14 00:47:20,853]\u001b[0m Trial 40 finished with value: 0.4102065189637711 and parameters: {'feature_fraction': 0.6799999999999999}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204:  67%|6| 4/6 [00:15<00:07,  3.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's binary_logloss: 0.399686\tvalid_1's binary_logloss: 0.410207\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399933\tvalid_1's binary_logloss: 0.41038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204:  83%|8| 5/6 [00:20<00:04,  4.0\u001b[32m[I 2023-02-14 00:47:25,161]\u001b[0m Trial 41 finished with value: 0.4102051734503014 and parameters: {'feature_fraction': 0.6479999999999999}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204:  83%|8| 5/6 [00:20<00:04,  4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's binary_logloss: 0.399611\tvalid_1's binary_logloss: 0.410205\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399989\tvalid_1's binary_logloss: 0.410483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.410204: 100%|#| 6/6 [00:23<00:00,  3.9\u001b[32m[I 2023-02-14 00:47:28,910]\u001b[0m Trial 42 finished with value: 0.4102765730202861 and parameters: {'feature_fraction': 0.52}. Best is trial 37 with value: 0.41020431590965467.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.410204: 100%|#| 6/6 [00:23<00:00,  3.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's binary_logloss: 0.399646\tvalid_1's binary_logloss: 0.410277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410204:   0%|   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410204:   5%| | 1/20 [00:03<01:13,  3.8\u001b[32m[I 2023-02-14 00:47:32,770]\u001b[0m Trial 43 finished with value: 0.41020431690859055 and parameters: {'lambda_l1': 0.0007773998922821829, 'lambda_l2': 3.2012859298995277e-06}. Best is trial 43 with value: 0.41020431690859055.\u001b[0m\n",
      "regularization_factors, val_score: 0.410204:   5%| | 1/20 [00:03<01:13,  3.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399937\tvalid_1's binary_logloss: 0.410374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  10%|1| 2/20 [00:08<01:15,  4.2\u001b[32m[I 2023-02-14 00:47:37,227]\u001b[0m Trial 44 finished with value: 0.41017609401570854 and parameters: {'lambda_l1': 6.616957066014342e-05, 'lambda_l2': 0.400853048601546}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  10%|1| 2/20 [00:08<01:15,  4.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.399532\tvalid_1's binary_logloss: 0.410198\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's binary_logloss: 0.399547\tvalid_1's binary_logloss: 0.410176\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  15%|1| 3/20 [00:12<01:10,  4.1\u001b[32m[I 2023-02-14 00:47:41,270]\u001b[0m Trial 45 finished with value: 0.41020431590955 and parameters: {'lambda_l1': 1.1027313099672533e-08, 'lambda_l2': 1.242001404761155e-07}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  15%|1| 3/20 [00:12<01:10,  4.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399936\tvalid_1's binary_logloss: 0.410371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  20%|2| 4/20 [00:17<01:10,  4.4\u001b[32m[I 2023-02-14 00:47:46,085]\u001b[0m Trial 46 finished with value: 0.41017818983276405 and parameters: {'lambda_l1': 0.010882827930218712, 'lambda_l2': 0.2708162972907513}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  20%|2| 4/20 [00:17<01:10,  4.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.399541\tvalid_1's binary_logloss: 0.410189\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid_0's binary_logloss: 0.399548\tvalid_1's binary_logloss: 0.410178\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  25%|2| 5/20 [00:20<01:02,  4.2\u001b[32m[I 2023-02-14 00:47:49,923]\u001b[0m Trial 47 finished with value: 0.410204314656062 and parameters: {'lambda_l1': 1.6996492507894156e-07, 'lambda_l2': 0.0014991323116035308}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  25%|2| 5/20 [00:21<01:02,  4.2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399942\tvalid_1's binary_logloss: 0.410385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  30%|3| 6/20 [00:24<00:55,  4.0\u001b[32m[I 2023-02-14 00:47:53,532]\u001b[0m Trial 48 finished with value: 0.4102478073054966 and parameters: {'lambda_l1': 1.0517138394360073, 'lambda_l2': 7.635176818135586e-07}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  30%|3| 6/20 [00:24<00:55,  4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's binary_logloss: 0.399718\tvalid_1's binary_logloss: 0.410248\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  35%|3| 7/20 [00:28<00:52,  4.0\u001b[32m[I 2023-02-14 00:47:57,734]\u001b[0m Trial 49 finished with value: 0.41020431591008577 and parameters: {'lambda_l1': 4.655367559816141e-07, 'lambda_l2': 9.449134137745608e-08}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  35%|3| 7/20 [00:28<00:52,  4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399943\tvalid_1's binary_logloss: 0.410368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  40%|4| 8/20 [00:32<00:47,  3.9\u001b[32m[I 2023-02-14 00:48:01,352]\u001b[0m Trial 50 finished with value: 0.41022949307154966 and parameters: {'lambda_l1': 9.490245203532942e-07, 'lambda_l2': 6.421168438428032}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  40%|4| 8/20 [00:32<00:47,  3.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's binary_logloss: 0.399722\tvalid_1's binary_logloss: 0.410229\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399937\tvalid_1's binary_logloss: 0.410377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  45%|4| 9/20 [00:36<00:42,  3.8\u001b[32m[I 2023-02-14 00:48:05,106]\u001b[0m Trial 51 finished with value: 0.41020262173015803 and parameters: {'lambda_l1': 0.2019055894080857, 'lambda_l2': 3.5275169933928286e-07}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  45%|4| 9/20 [00:36<00:42,  3.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399625\tvalid_1's binary_logloss: 0.410203\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399937\tvalid_1's binary_logloss: 0.410374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  50%|5| 10/20 [00:39<00:38,  3.\u001b[32m[I 2023-02-14 00:48:08,783]\u001b[0m Trial 52 finished with value: 0.4102117881813872 and parameters: {'lambda_l1': 0.22183125618514202, 'lambda_l2': 2.9286247167445133e-06}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  50%|5| 10/20 [00:39<00:38,  3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's binary_logloss: 0.399666\tvalid_1's binary_logloss: 0.410212\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  55%|5| 11/20 [00:44<00:36,  4.\u001b[32m[I 2023-02-14 00:48:13,246]\u001b[0m Trial 53 finished with value: 0.4102043068763135 and parameters: {'lambda_l1': 3.005784364160781e-05, 'lambda_l2': 0.012016387113167529}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  55%|5| 11/20 [00:44<00:36,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399937\tvalid_1's binary_logloss: 0.410374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  60%|6| 12/20 [00:48<00:31,  3.\u001b[32m[I 2023-02-14 00:48:17,128]\u001b[0m Trial 54 finished with value: 0.4102116872979885 and parameters: {'lambda_l1': 0.0018771094002443328, 'lambda_l2': 0.5269960161354045}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  60%|6| 12/20 [00:48<00:31,  3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's binary_logloss: 0.399666\tvalid_1's binary_logloss: 0.410212\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  65%|6| 13/20 [00:52<00:27,  3.\u001b[32m[I 2023-02-14 00:48:21,011]\u001b[0m Trial 55 finished with value: 0.4101905103102836 and parameters: {'lambda_l1': 0.0055598266799962775, 'lambda_l2': 0.08848539231015028}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  65%|6| 13/20 [00:52<00:27,  3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399616\tvalid_1's binary_logloss: 0.410191\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399944\tvalid_1's binary_logloss: 0.410368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  70%|7| 14/20 [00:56<00:24,  4.\u001b[32m[I 2023-02-14 00:48:25,287]\u001b[0m Trial 56 finished with value: 0.4102044355856427 and parameters: {'lambda_l1': 5.532876810755366e-05, 'lambda_l2': 8.188811123463955}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  70%|7| 14/20 [00:56<00:24,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[188]\tvalid_0's binary_logloss: 0.399582\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  75%|7| 15/20 [01:00<00:20,  4.\u001b[32m[I 2023-02-14 00:48:29,258]\u001b[0m Trial 57 finished with value: 0.4102043369688771 and parameters: {'lambda_l1': 0.016465462561208268, 'lambda_l2': 0.00016775935228319523}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  75%|7| 15/20 [01:00<00:20,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399936\tvalid_1's binary_logloss: 0.410371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  80%|8| 16/20 [01:04<00:16,  4.\u001b[32m[I 2023-02-14 00:48:33,442]\u001b[0m Trial 58 finished with value: 0.4101781573218731 and parameters: {'lambda_l1': 0.00011097072332528694, 'lambda_l2': 0.3400184035466205}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  80%|8| 16/20 [01:04<00:16,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.399541\tvalid_1's binary_logloss: 0.410189\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid_0's binary_logloss: 0.399548\tvalid_1's binary_logloss: 0.410178\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399935\tvalid_1's binary_logloss: 0.410375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410176:  85%|8| 17/20 [01:08<00:11,  3.\u001b[32m[I 2023-02-14 00:48:37,132]\u001b[0m Trial 59 finished with value: 0.4102043011190775 and parameters: {'lambda_l1': 8.834117816440825e-05, 'lambda_l2': 0.021745805891122107}. Best is trial 44 with value: 0.41017609401570854.\u001b[0m\n",
      "regularization_factors, val_score: 0.410176:  85%|8| 17/20 [01:08<00:11,  3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's binary_logloss: 0.399619\tvalid_1's binary_logloss: 0.410204\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399958\tvalid_1's binary_logloss: 0.410408\n",
      "[200]\tvalid_0's binary_logloss: 0.39957\tvalid_1's binary_logloss: 0.410176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410170:  90%|9| 18/20 [01:12<00:08,  4.\u001b[32m[I 2023-02-14 00:48:41,503]\u001b[0m Trial 60 finished with value: 0.4101699907179804 and parameters: {'lambda_l1': 8.923803146534777, 'lambda_l2': 0.42180245617091994}. Best is trial 60 with value: 0.4101699907179804.\u001b[0m\n",
      "regularization_factors, val_score: 0.410170:  90%|9| 18/20 [01:12<00:08,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.399559\tvalid_1's binary_logloss: 0.41017\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410163:  95%|9| 19/20 [01:17<00:04,  4.\u001b[32m[I 2023-02-14 00:48:46,276]\u001b[0m Trial 61 finished with value: 0.4101627223864833 and parameters: {'lambda_l1': 6.226008496644099, 'lambda_l2': 1.5919572018526484}. Best is trial 61 with value: 0.4101627223864833.\u001b[0m\n",
      "regularization_factors, val_score: 0.410163:  95%|9| 19/20 [01:17<00:04,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.399958\tvalid_1's binary_logloss: 0.410396\n",
      "[200]\tvalid_0's binary_logloss: 0.399575\tvalid_1's binary_logloss: 0.410177\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's binary_logloss: 0.399567\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.410163: 100%|#| 20/20 [01:21<00:00,  4.\u001b[32m[I 2023-02-14 00:48:50,561]\u001b[0m Trial 62 finished with value: 0.4101717281092271 and parameters: {'lambda_l1': 8.295179343824277, 'lambda_l2': 3.2297470955004814}. Best is trial 61 with value: 0.4101627223864833.\u001b[0m\n",
      "regularization_factors, val_score: 0.410163: 100%|#| 20/20 [01:21<00:00,  4.\n",
      "min_data_in_leaf, val_score: 0.410163:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.410163:  20%|4 | 1/5 [00:04<00:17,  4.26s/it]\u001b[32m[I 2023-02-14 00:48:54,838]\u001b[0m Trial 63 finished with value: 0.4101627223864833 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.4101627223864833.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.410163:  20%|4 | 1/5 [00:04<00:17,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.410163:  40%|8 | 2/5 [00:08<00:12,  4.29s/it]\u001b[32m[I 2023-02-14 00:48:59,152]\u001b[0m Trial 64 finished with value: 0.4101627223864833 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.4101627223864833.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.410163:  40%|8 | 2/5 [00:08<00:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.410163:  60%|#2| 3/5 [00:13<00:08,  4.37s/it]\u001b[32m[I 2023-02-14 00:49:03,618]\u001b[0m Trial 65 finished with value: 0.4101627223864833 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.4101627223864833.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.410163:  60%|#2| 3/5 [00:13<00:08,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.410163:  80%|#6| 4/5 [00:17<00:04,  4.47s/it]\u001b[32m[I 2023-02-14 00:49:08,227]\u001b[0m Trial 66 finished with value: 0.4101627223864833 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.4101627223864833.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.410163:  80%|#6| 4/5 [00:17<00:04,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n",
      "[LightGBM] [Info] Number of positive: 95176, number of negative: 357183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 452359, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.210399 -> initscore=-1.322520\n",
      "[LightGBM] [Info] Start training from score -1.322520\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.39995\tvalid_1's binary_logloss: 0.41038\n",
      "[200]\tvalid_0's binary_logloss: 0.399568\tvalid_1's binary_logloss: 0.410172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.410163: 100%|##| 5/5 [00:22<00:00,  4.43s/it]\u001b[32m[I 2023-02-14 00:49:12,593]\u001b[0m Trial 67 finished with value: 0.4101627223864833 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.4101627223864833.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.410163: 100%|##| 5/5 [00:22<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's binary_logloss: 0.399553\tvalid_1's binary_logloss: 0.410163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = split_data(df)\n",
    "# 不要なカラムの削除と、目的変数の切り分け（単勝オッズは抜いてみた）\n",
    "X_train = train_data.drop(['date', 'rank'], axis=1)\n",
    "y_train = train_data['rank']\n",
    "X_valid = valid_data.drop(['date', 'rank'], axis=1)\n",
    "y_valid = valid_data['rank']\n",
    "\n",
    "#データセットを作成\n",
    "lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n",
    "lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary', #今回は0or1の二値予測なのでbinaryを指定\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "#チューニング実行\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train,\n",
    "                        valid_sets=(lgb_train, lgb_valid),\n",
    "                        verbose_eval=100,\n",
    "                        early_stopping_rounds=10,\n",
    "                        optuna_seed=100 #optunaのseed固定\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "827e7028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 6.226008496644099,\n",
       " 'lambda_l2': 1.5919572018526484,\n",
       " 'num_leaves': 3,\n",
       " 'feature_fraction': 0.552,\n",
       " 'bagging_fraction': 0.8870098894544057,\n",
       " 'bagging_freq': 2,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5607dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashigeyuki/opt/anaconda3/envs/horse/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.552, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.552\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8870098894544057, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8870098894544057\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5919572018526484, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5919572018526484\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.226008496644099, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.226008496644099\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.8870098894544057, bagging_freq=2,\n",
       "               feature_fraction=0.552, feature_pre_filter=False,\n",
       "               lambda_l1=6.226008496644099, lambda_l2=1.5919572018526484,\n",
       "               num_iterations=1000, num_leaves=3, objective=&#x27;binary&#x27;,\n",
       "               random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(bagging_fraction=0.8870098894544057, bagging_freq=2,\n",
       "               feature_fraction=0.552, feature_pre_filter=False,\n",
       "               lambda_l1=6.226008496644099, lambda_l2=1.5919572018526484,\n",
       "               num_iterations=1000, num_leaves=3, objective=&#x27;binary&#x27;,\n",
       "               random_state=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.8870098894544057, bagging_freq=2,\n",
       "               feature_fraction=0.552, feature_pre_filter=False,\n",
       "               lambda_l1=6.226008496644099, lambda_l2=1.5919572018526484,\n",
       "               num_iterations=1000, num_leaves=3, objective='binary',\n",
       "               random_state=100)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = lgb_clf_o.params.copy()\n",
    "del(params['early_stopping_round'])\n",
    "\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4dd68d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = './pickle/model.pkl'\n",
    "pickle.dump(lgb_clf, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0064d10",
   "metadata": {},
   "source": [
    "# 騎手・馬のラベル辞書データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "81d605ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "jockey_dict = dict()\n",
    "for i in range(len(train_data)):\n",
    "    jockey_dict[train_data['jockey_id'].iloc[i]] = train_data['jockey_label'].iloc[i]\n",
    "    \n",
    "horse_dict = dict()\n",
    "for i in range(len(train_data)):\n",
    "    horse_dict[train_data['horse_id'].iloc[i]] = train_data['horse_label'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "374a9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./pickle/jockey_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(jockey_dict, f)\n",
    "    \n",
    "with open(\"./pickle/horse_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(horse_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b880b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
